import Foundation
import Vision
import UIKit

// MARK: - OCRè¯†åˆ«ç»“æœ
struct OCRResult {
    let recognizedText: String
    let confidence: Float
    let boundingBox: CGRect?
    
    init(text: String, confidence: Float = 0.0, boundingBox: CGRect? = nil) {
        self.recognizedText = text
        self.confidence = confidence
        self.boundingBox = boundingBox
    }
}

// MARK: - OCRè¯†åˆ«æœåŠ¡
class OCRService {
    
    // MARK: - å•ä¾‹æ¨¡å¼
    static let shared = OCRService()
    
    private init() {}
    
    // MARK: - è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
    func recognizeText(from image: UIImage, completion: @escaping (Result<[OCRResult], Error>) -> Void) {
        print("ğŸ” å¼€å§‹OCRè¯†åˆ«...")
        print("ğŸ“· å›¾ç‰‡ä¿¡æ¯: å°ºå¯¸=\(image.size), æ–¹å‘=\(image.imageOrientation.rawValue)")
        
        guard let cgImage = image.cgImage else {
            print("âŒ æ— æ³•è·å–CGImage")
            completion(.failure(OCRError.invalidImage))
            return
        }
        
        print("ğŸ“· CGImageä¿¡æ¯: å®½=\(cgImage.width), é«˜=\(cgImage.height)")
        
        // åˆ›å»ºæ–‡å­—è¯†åˆ«è¯·æ±‚
        let request = VNRecognizeTextRequest { (request, error) in
            if let error = error {
                print("âŒ OCRè¯†åˆ«è¯·æ±‚å¤±è´¥: \(error.localizedDescription)")
                completion(.failure(error))
                return
            }
            
            guard let observations = request.results as? [VNRecognizedTextObservation] else {
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬è¯†åˆ«ç»“æœ")
                completion(.failure(OCRError.noTextFound))
                return
            }
            
            print("ğŸ“ è¯†åˆ«åˆ° \(observations.count) ä¸ªæ–‡æœ¬åŒºåŸŸ")
            let results = self.processTextObservations(observations)
            print("âœ… OCRè¯†åˆ«å®Œæˆï¼Œå…±è¯†åˆ«åˆ° \(results.count) ä¸ªæ–‡æœ¬å—")
            
            // æ‰“å°æ‰€æœ‰è¯†åˆ«ç»“æœ
            for (index, result) in results.enumerated() {
                print("ğŸ“ æ–‡æœ¬[\(index)]: '\(result.recognizedText)' (ç½®ä¿¡åº¦: \(String(format: "%.2f", result.confidence)))")
            }
            
            completion(.success(results))
        }
        
        // é…ç½®è¯†åˆ«å‚æ•° - ä½¿ç”¨æœ€é«˜ç²¾åº¦
        print("ğŸ”§ OCRé…ç½®: ç²¾ç¡®æ¨¡å¼, è¯­è¨€=[zh-CN, en], è¯­è¨€æ ¡æ­£=å¼€å¯")
        request.recognitionLevel = .accurate
        request.recognitionLanguages = ["zh-CN", "en"] // æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡
        request.usesLanguageCorrection = true
        
        // æ‰§è¡Œè¯†åˆ«
        print("ğŸš€ å¼€å§‹æ‰§è¡ŒOCRè¯†åˆ«...")
        let requestHandler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                try requestHandler.perform([request])
            } catch {
                print("âŒ OCRæ‰§è¡Œå¤±è´¥: \(error.localizedDescription)")
                DispatchQueue.main.async {
                    completion(.failure(error))
                }
            }
        }
    }
    
    // MARK: - å¤„ç†è¯†åˆ«ç»“æœ
    private func processTextObservations(_ observations: [VNRecognizedTextObservation]) -> [OCRResult] {
        var results: [OCRResult] = []
        
        for observation in observations {
            guard let topCandidate = observation.topCandidates(1).first else { continue }
            
            let result = OCRResult(
                text: topCandidate.string,
                confidence: topCandidate.confidence,
                boundingBox: observation.boundingBox
            )
            
            results.append(result)
        }
        
        return results.sorted { $0.confidence > $1.confidence }
    }
    
    // MARK: - ä»ç›¸æœºæˆ–ç›¸å†Œè·å–çš„å›¾ç‰‡è¿›è¡Œè¯†åˆ«
    func recognizeTextFromImageData(_ imageData: Data, completion: @escaping (Result<String, Error>) -> Void) {
        guard let image = UIImage(data: imageData) else {
            completion(.failure(OCRError.invalidImage))
            return
        }
        
        recognizeText(from: image) { result in
            DispatchQueue.main.async {
                switch result {
                case .success(let ocrResults):
                    let fullText = ocrResults.map { $0.recognizedText }.joined(separator: "\n")
                    completion(.success(fullText))
                case .failure(let error):
                    completion(.failure(error))
                }
            }
        }
    }
    
    // MARK: - é¢„å¤„ç†å›¾ç‰‡ï¼ˆæé«˜è¯†åˆ«å‡†ç¡®ç‡ï¼‰
    func preprocessImage(_ image: UIImage) -> UIImage? {
        guard let cgImage = image.cgImage else { return nil }
        
        let context = CIContext()
        let ciImage = CIImage(cgImage: cgImage)
        
        // å¢å¼ºå¯¹æ¯”åº¦
        guard let contrastFilter = CIFilter(name: "CIColorControls") else { return image }
        contrastFilter.setValue(ciImage, forKey: kCIInputImageKey)
        contrastFilter.setValue(1.5, forKey: kCIInputContrastKey) // å¢åŠ å¯¹æ¯”åº¦
        contrastFilter.setValue(0.1, forKey: kCIInputBrightnessKey) // ç¨å¾®å¢åŠ äº®åº¦
        
        guard let contrastOutput = contrastFilter.outputImage else { return image }
        
        // é”åŒ–
        guard let sharpenFilter = CIFilter(name: "CISharpenLuminance") else { return image }
        sharpenFilter.setValue(contrastOutput, forKey: kCIInputImageKey)
        sharpenFilter.setValue(0.8, forKey: kCIInputSharpnessKey)
        
        guard let finalOutput = sharpenFilter.outputImage,
              let processedCGImage = context.createCGImage(finalOutput, from: finalOutput.extent) else {
            return image
        }
        
        return UIImage(cgImage: processedCGImage)
    }
}

// MARK: - OCRé”™è¯¯å®šä¹‰
enum OCRError: LocalizedError {
    case invalidImage
    case noTextFound
    case processingFailed
    
    var errorDescription: String? {
        switch self {
        case .invalidImage:
            return "æ— æ•ˆçš„å›¾ç‰‡"
        case .noTextFound:
            return "æœªæ‰¾åˆ°å¯è¯†åˆ«çš„æ–‡å­—"
        case .processingFailed:
            return "å›¾ç‰‡å¤„ç†å¤±è´¥"
        }
    }
}

// MARK: - æ‰©å±•æ–¹æ³•
extension OCRService {
    
    // MARK: - æ‰¹é‡è¯†åˆ«å¤šå¼ å›¾ç‰‡
    func recognizeTextFromImages(_ images: [UIImage], completion: @escaping (Result<[String], Error>) -> Void) {
        let group = DispatchGroup()
        var results: [String] = []
        var hasError: Error?
        
        for image in images {
            group.enter()
            recognizeText(from: image) { result in
                defer { group.leave() }
                
                switch result {
                case .success(let ocrResults):
                    let text = ocrResults.map { $0.recognizedText }.joined(separator: "\n")
                    results.append(text)
                case .failure(let error):
                    hasError = error
                }
            }
        }
        
        group.notify(queue: .main) {
            if let error = hasError {
                completion(.failure(error))
            } else {
                completion(.success(results))
            }
        }
    }
    
    // MARK: - æ£€æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«é‡‘é¢ä¿¡æ¯
    func containsAmount(_ text: String) -> Bool {
        let amountPatterns = [
            "\\d+\\.?\\d*å…ƒ",
            "Â¥\\d+\\.?\\d*",
            "\\d+\\.?\\d*[å…ƒå—]",
            "æ€»é‡‘é¢.*\\d+",
            "é‡‘é¢.*\\d+",
            "æ”¯ä»˜.*\\d+"
        ]
        
        return amountPatterns.contains { pattern in
            text.range(of: pattern, options: .regularExpression) != nil
        }
    }
    
    // MARK: - æ£€æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«å•†å®¶ä¿¡æ¯
    func containsMerchant(_ text: String) -> Bool {
        let merchantKeywords = ["æ”¶æ¬¾æ–¹", "å•†å®¶", "åº—é“º", "è¶…å¸‚", "é¤å…", "è¯åº—", "åŠ æ²¹ç«™", "ä¾¿åˆ©åº—"]
        return merchantKeywords.contains { text.contains($0) }
    }
} 